{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eb25b54-ad3e-4c6d-bc73-1d17802243e4",
   "metadata": {},
   "source": [
    "## 2.5 Tokenizing Text\n",
    "\n",
    "Fundamental step in NLP involves converting our text into smaller units through a process known as tokenization. These smaller units are known as our tokens. Word tokenization is the most common form of tokenization, where individual words in the text becomes a token, but tokens can also be sentences, sub words or individual characters depending on your use case.\n",
    "\n",
    "Why do we do this? The meaning of the overall text is better understood if we can analyse and understand the individual parts as well as the whole. It's also an important step before we vecotrize our data, which we'll cover more in the next section of this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83748713-ca0c-4dcf-94f1-705f019fa2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/omergamie/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7bcb1d-40de-4ca5-810a-39bedf5123ba",
   "metadata": {},
   "source": [
    "### Sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38aa6f0d-5cb2-4af2-bb7e-5bf065869ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = \"Her cat's name is Luna. Her dog's name is Max\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd3e639b-bec1-4769-a965-029e79d9ec6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Her cat's name is Luna.\", \"Her dog's name is Max\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d499975-1af2-47bf-aae7-4bd7c286e57c",
   "metadata": {},
   "source": [
    "### Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3743b82b-ff33-4480-8ae4-f5bcb4c00d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Her',\n",
       " 'cat',\n",
       " \"'s\",\n",
       " 'name',\n",
       " 'is',\n",
       " 'Luna',\n",
       " '.',\n",
       " 'Her',\n",
       " 'dog',\n",
       " \"'s\",\n",
       " 'name',\n",
       " 'is',\n",
       " 'Max']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a93b647-1ec5-4754-802f-db27e485e10f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
